\chapter{Appendix}





\section{Method Details}

% We draw our concepts from the WordNet hierarchy~\cite{miller1995wordnet}, which consists of $146{,}347$ noun lemmas. Not all of these lemmas are visual, but the vocabulary still covers an incredible range of topics.
% % For reference, here are 6 randomly sampled concepts:  {\tt {\small `sleep talking', 
% % `beach wagon', `Balearic Islands', `borosilicate', `genus Loranthus', `humpback whale'}}.
% We can generate descriptors for each concept by prompting a GPT-J language model~\cite{gpt-j} with examples of descriptor-concept pairs (details in the supplementary).
% % For reference, here are 7 randomly sampled descriptors for ``labrador retriever'': {\tt {\small `friendly', `short', `long-legged', `big', `fast', `blue-eyed', `handsome'}}.

\subsection{WordNet Lemmas}
\label{sec:wordnet_lemmas}
We draw our concepts from the WordNet hierarchy~\cite{miller1995wordnet}, which consists of $146{,}347$ noun lemmas. For reference, here are 32 randomly sampled concepts:
\begin{quote}
{\tt { 
% `sleep talking', `beach wagon', `Balearic Islands', `borosilicate', `genus Loranthus', `humpback whale'
"resolution",
"lodgment",
"phycobilin",
"acidosis",
"widening",
"human face",
"family Crassulaceae",
"sail",
"Ipomoea imperialis",
"Davis",
"prothrombin",
"cease",
"marsh clematis",
"major power",
"chump change",
"madcap",
"junky",
"pere david's deer",
"make-up",
"genus Rumex",
"gape",
"Brachychiton populneus",
"bell morel",
"wain",
"friendly",
"Principe",
"bottle green",
"glycerol trimargarate",
"water-shield",
"San Joaquin River",
"woodsman",
"pin".
}}
\end{quote}

\subsection{GPT-J Descriptor Prompting}
\label{sec:gptj-descriptors}
We use GPT-J-6B~\cite{gpt-j}, a free, open-source autoregressive language model, to generate useful descriptors for a given concept. We use the following prompt template: 
\begin{itemize}
    \item[] \texttt{"What are some words that describe the quality of `\{concept\}'?} 
    \item[] \texttt{The \{concept\} is frail.}
    \item[] \texttt{The \{concept\} is red.}
    \item[] \texttt{The \{concept\} is humongous.}
    \item[] \texttt{The \{concept\} is tall.}
    \item[] \texttt{The \{concept\} is"}
\end{itemize}

We sample completions with a temperature of 0.9 and a max length of 100 tokens. We truncate the completion after the first comma, period, underscore, or newline character (including the special character). If the truncated completion is degenerate and contains a duplicate of the concept, we resample another completion. After successfully sampling a descriptor, we prepend it to the concept and use the resulting phrase as our search query. 

% - TODO: put in a page of examples.   
% - Pencil: 'fragile', 'huge!', 'too big', 'colorful', 'dark', 'too soft', 'blunt', 'narrow', 'round', 
% 'blue', 'black', 'a soft piece of wood', 'thick', 'heavy', 'light', 'short'
% - TODO: These descriptors generally 
% - Our prompt can be easily 

For reference, here are 32 randomly sampled descriptors for ``labrador retriever'':
% {\tt {\small `friendly', `short', `long-legged', `big', `fast', `blue-eyed', `handsome'}}.
\begin{quote}
{\tt { 
% `sleep talking', `beach wagon', `Balearic Islands', `borosilicate', `genus Loranthus', `humpback whale'
"a good-looking dog",
"very gentle",
"a",
"brown",
"lovable",
"a strong runner",
"a male or a female",
"sturdy",
"agile",
"a strong",
"beautiful",
"a male",
"kind",
"long-haired",
"a male or a female",
"a good-looking dog",
"gentle",
"medium",
"loyal",
"very gentle",
"blue-eyed",
"sturdy",
"blue-eyed",
"a retriever",
"kind",
"loyal",
"large",
"brown",
"good-natured",
"gentle",
"large",
"small".
}}
\end{quote}



\subsection{Concept Vocabulary Size}
\label{sec:concept_vocab_size}
As stated in Section \ref{subsec:text_query_generation}, our vocabulary comprises the $146{,}347$ noun lemmas in the WordNet hierarchy. Thus, in all our experiments, Internet Explorer only searches for WordNet terms (plus the class names, if we have knowledge of the label set). We found that this worked quite well for these standard benchmarks. Note that expanding the vocabulary (e.g., adding technical terms relevant to a specific topic) can easily be done by adding those terms to the list of possible concepts. One easy extension would be to add page titles and frequent unigrams and bigrams from Wikipedia, as was done to generate the CLIP training set~\cite{radford2021learning}. Doing so would expand our vocabulary to roughly $500{,}000$ total concepts. 

\subsection{Query Model Details}
\label{sec:query_model_details}
\paragraph{Temperature for concept distribution}
After estimating scores $r(c_i)$ for each concept $c_i$, we do a temperature-scaled softmax, followed by the tiering operation described in Section 2.6. We compute the temperature $\tau$ such that 
\begin{align}
     \text{SMR} = \frac{\max_i r(c_i) - \min_i r(c_i)}{\tau}
\end{align}
where the ``softmax range'' $\text{SMR} \in \mathbb R$ is the desired gap between the largest and smallest scores after temperature scaling. After the softmax $p(c_i) \propto \exp(r(c_i) / \tau)$, the softmax range determines the likelihood ratio of most likely concept to least likely concept: 
\begin{align}
    \frac{\max_i p(c_i)}{\min_i p(c_i)} &= \frac{\max_i \exp(r(c_i) / \tau)}{\min_i \exp(r(c_i) / \tau)} \\
      &= \exp \left(\frac{\max_i r(c_i) - \min_i r(c_i)}{\tau}\right) \\
    &= \exp(\text{SMR})
\end{align}
Thus, SMR is an easy way to specify the relative likelihood of the highest and lowest scoring concepts and achieve a desired exploration-exploitation balance. 


\paragraph{Label set-guided vocabulary}
% When we know the label set, 
% TODO(Ellis) describe the vocab subset creation for each dataset 
To reduce our search space in the label set-guided setting, in which we know the English names of the classes a priori, we generate a subset of the WordNet vocabulary that contains only the top-$10\%$ most semantically-relevant concepts to each target dataset.
We use a pre-trained text embedding model~\cite{reimers2019sentence} to generate $384$-dimensional embeddings for each concept in WordNet, using the same template described in Section 2.5 of the main paper: %, and reproduced below:

\begin{quote}
\vspace{-0.07in}
{\tt {\small \{lemma\} (\{hypernym\}): \{definition\}}}.
\vspace{-0.07in}
\end{quote}

To generate a similar embedding for concepts in target datasets, we use the summary from Wikipedia in place of the definition and the ``category'' of the target dataset (shown in \cref{tab:dataset_categories}) in place of the hypernym:

\begin{quote}
\vspace{-0.07in}
{\tt {\small \{label\} (\{category\}): \{summary\}}}.
\vspace{-0.07in}
\end{quote}


\begin{table}
    \centering
    \begin{tabular}{ll}
    \toprule
        Dataset & Category \\
    \midrule
        Oxford Flowers102 & Flower \\
        Oxford IIIT Pets & Pet \\
        Food101 & Food \\
        Birdsnap & Bird \\
        VOC2007 & Object \\
    \bottomrule
    \end{tabular}
    \caption{\textbf{Target Dataset ``Category''}.
    }
    \label{tab:dataset_categories}
\end{table}

After generating the embeddings for each concept in the target dataset, we find the $k$-NN distance for each WordNet concept to the target dataset embeddings, where $k$ is chosen to be $1/3$ the size of the class label set.
We then rank the concepts in WordNet by the distance and take the closest $10\%$ of terms as our subset. This subset is used for all methods in the label set-guided setting, including the random exploration methods. 






\subsection{Training Details}
In each iteration, we download roughly 25k candidate images, since we download up to 100 images for each of the 256 queries. Given this set $\mathcal C$ of candidate images, we sample $\text{PCR} \times |\mathcal C|$ images from the union of the replay buffer $\mathcal B$ and the target dataset training images $\mathcal D$. PCR (past data to candidate data ratio) is a scalar value that determines how much old data vs new data to train on at every iteration. We set $\text{PCR}=2$ for all experiments. We perform $10$ epochs of training over the union of the new candidate data and the sampled replay buffer and target dataset images. 

\subsection{Hyperparameters}
% privilege ratio, privilege gamma 
% k 
% GP 

\cref{tab:hyperparameters} shows our hyperparameter values, which are shared across datasets. We perform minimal hyperparameter tuning and copy most of the values from the MoCo-v3~\cite{chen2021empirical} ResNet-50 configuration. 
We will also release our code upon acceptance, which we hope will clarify any remaining implementation details and make it easy for the community to reproduce and build on our work. 
\begin{table}
    \centering
    % \begin{adjustbox}{width=\linewidth}
    \begin{tabular}{ll}
    \toprule
        Hyperparameter & Value \\
    \midrule
        Architecture & Resnet-50~\cite{he2016deep} \\
        Optimizer & LARS~\cite{you2017large} \\
        Batch size & $224$ \\
        Learning rate & $0.8 \times \frac{224}{256}$ \\
        Learning rate schedule & constant \\
        MoCo momentum & $0.9985$ \\
        RandomResizedCrop min crop area & $0.2$ \\
        % PCR & 2? \\
        Queries per iteration & $256$ \\
        Requested images per query & $100$ \\
        Min images per query & $10$ \\    
        Softmax range (SMR) & $3$ \\
        PCR & $2$ \\
        Epochs per iteration & $10$ \\
    \bottomrule
    \end{tabular}
    % \end{adjustbox}
    \caption{\textbf{Internet Explorer hyperparameters}.}
    \label{tab:hyperparameters}
\end{table}


\subsection{Image Licenses}
Internet Explorer uses images that were indexed by a web crawler (Google Images and LAION) or uploaded to Flickr. The images and their rights belong to their respective owners; we use, download, and train on them under fair use guidelines for research. 

% \subsection{Google Image Bias}
% \subsection{More MoCo loss failure cases?}
% \begin{itemize}
%     \item long images 
% \end{itemize}



% \section{VOC2007 Linear Probe Accuracy}
% \begin{table}[t]
%     \centering
%     \begin{tabular}{lc}
%     \toprule
%         Model & VOC2007 \\
%     \midrule
%     \textit{Fixed dataset Self-Supervised} \\
%         \;\;\; MoCo-v3~\cite{chen2021empirical} (VOC2007 pre-train)   & 58.0 \\
%     \midrule
%     \textit{No label set information} \\
%         \;\;\;Random exploration                                      &  64.8 \\ % expID 1138
%         \;\;\;Ours                                                    & 63.4 \\ % expID 1139
%         \;\;\;Ours++                                                  & 64.9 \\ % expID 1142
%     \midrule 
%     \textit{Use label set information} \\       
%         \;\;\;Search labels only                                      & 60.8 \\ % expID 1141
%         \;\;\;Labels + semantically relevant terms                    & 65.3 \\ % expID 1136
%         \;\;\;Ours                                                    & 65.4 \\ % expID 1140
%         \;\;\;Ours++                                                  & \textbf{72.9} \\ % expID 1143
%     \bottomrule
%     \end{tabular}
%     \caption{\textbf{Linear probe accuracy on VOC2007}. 
%     }
%     \label{tab:voc2007}
% \end{table}

% VOC categories are broad, unlike the fine-grained classes in Birdsnap, Flowers, Food, and Pets. Thus, comparing against ImageNet pre-training or CLIP would be unfair since these datasets are diverse, cleaned, class-balanced, and entirely relevant to VOC. Thus, we start from and compare against a MoCo-v3 checkpoint that has only been pre-trained on VOC2007. We report the linear probe accuracy for Pascal VOC2007 in \cref{tab:voc2007}. We find that Internet Explorer improves faster than randomly searching the Internet, especially when it has label set information to help guide it. 

% \section{More seeds}



\section{Progression of downloaded images}
\label{sec:progression_downloaded_imgs}
Just as Fig. 4 in the main paper showed how Internet Explorer progressively discovers useful data when targeting the Pets dataset, \cref{fig:birdsnap_progression}, \cref{fig:flowers_progression}, \cref{fig:food_progression}, and \cref{fig:voc_progression} show the progression of downloaded images when targeting Birdsnap, Flowers, Food, and VOC respectively. Note that this analysis is in the self-supervised setting, without any knowledge of the label set. 

\begin{figure*}
    \centering
    \includegraphics{figures/birdsnap_targets.pdf} \\
    \vspace{-0.8em}
    \includegraphics{figures/birdsnap-progression-1146-2col.pdf}
    \caption{\textbf{Progression of downloaded Birdsnap images.} This corresponds to Ours++ without using label set information. }
    \label{fig:birdsnap_progression}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics{figures/flowers_targets.pdf} \\
    \vspace{-0.8em}
    \includegraphics{figures/flowers-progression-1150-2col.pdf}
    \caption{\textbf{Progression of downloaded Flowers images.} This corresponds to Ours++ without using label set information. }
    \label{fig:flowers_progression}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics{figures/food_targets.pdf} \\
    \vspace{-0.8em}
    \includegraphics{figures/food-progression-1148-2col.pdf}
    \caption{\textbf{Progression of downloaded Food images.} This corresponds to Ours++ without using label set information. }
    \label{fig:food_progression}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics{figures/voc_targets.pdf} \\
    \vspace{-0.8em}
    \includegraphics{figures/voc-progression-1156-2col.pdf}
    \caption{\textbf{Progression of downloaded VOC2007 images.} This corresponds to Ours++ without using label set information. }
    \label{fig:voc_progression}
\end{figure*}

% \section{Other experiments:}
% \begin{itemize}
% \item do we need GP or not 
% \item do we need std + mean or not 
% \end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Additional Figures}
\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{figures/reward_ranking.pdf}
    % \vspace{-0.25in}
    \caption{\textbf{Top images preferred by different rewards.} We show the top 5 downloaded images ranked by 3 possible image rewards on the Food dataset. 15-NN (ours) prefers a variety of food images, whereas MoCo prefers noisy images out of the training distribution. 1-NN is thrown off by outliers in the Food dataset and thus prefers black images, text, and zebras.}
    \label{fig:reward_ranking}
    % \vspace{-0.06in}
\end{figure}

%%% Local Variables:
%%% coding: utf-8
%%% mode: latex
%%% TeX-engine: xetex
%%% TeX-master: "../thesis"
%%% End: