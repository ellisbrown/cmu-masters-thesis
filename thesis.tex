\PassOptionsToPackage{svgnames,dvipsnames}{xcolor}

%% add the masters option to print master of science
\documentclass[12pt,masters]{cmuthesis}

\usepackage[Lenny]{fncychap}
\ChNameVar{\Large}

\input{sections/packages}
\input{sections/macros}

\draftstamp{\today}{DRAFT}

\begin{document}
\frontmatter

\pagestyle{empty}

\title{{\bf Online Representation Learning on the Open Web}}
\author{Ellis L. Brown, II}
\date{May 2023}
\Year{2023}
\trnumber{CMU-CS-XX-XXX}

\committee{
    \begin{tabular}{rl}
        Deepak Pathak           & \textit{Carnegie Mellon University} \\
        Deva Ramanan            & \textit{Carnegie Mellon University} \\
        Alexei A. Efros         & \textit{University of California, Berkeley} \\
    \end{tabular}
}

\support{}
\disclaimer{}

\keywords{
    machine learning,
    deep learning,
    computer vision,
    representation learning,
    self-supervised learning,
    active learning,
    online learning,
    internet
}

\maketitle

\begin{dedication}
    To all of the people that light up my life. {\ensuremath\heartsuit}
\end{dedication}

\begin{abstract}
    Domain-specific modeling priors and specialized components are
    becoming increasingly important to the machine learning field.
    These components integrate specialized knowledge that we have
    as humans into model.
    We argue in this thesis that optimization methods provide an
    expressive set of operations that should be part of the
    machine learning practitioner's modeling toolbox.

    We present two foundational approaches for optimization-based modeling:
    1) the \emph{OptNet} architecture that integrates
    optimization problems as individual layers in larger end-to-end
    trainable deep networks, and
    2) the \emph{input-convex neural network (ICNN)}
    architecture that helps make inference and learning in deep
    energy-based models and structured prediction more tractable.

    We then show how to use the OptNet approach
    1) as a way of combining model-free and model-based reinforcement
    learning and
    2) for top-$k$ learning problems.
    We conclude by showing how to differentiate cone programs
    and turn the \cvxpy domain specific language into
    a differentiable optimization layer that enables rapid prototyping of
    the approaches in this thesis. \\

    \noindent
    The source code for this thesis document is available in open source form at:
    \begin{center}
        \url{https://github.com/ellisbrown/cmu-masters-thesis}
    \end{center}
\end{abstract}

% \newgeometry{left=0.5in,right=0.5in,top=1in,bottom=1.4in}
\begin{acknowledgments}
    I have been incredibly fortunate and privileged throughout
    my entire life to have been given many opportunities
    that have led me to pursue this thesis research.
    Thanks to the thousands of people in the universe throughout
    the past few millennia who have provided me with the
    foundation, environment, safety, health, support, service,
    financial well-being, love, joy, knowledge, kindness, calmness,
    and happiness to produce this work.

    This thesis would not have been possible without the close
    collaboration I have had with my advisor J.~Zico Kolter over
    the past few years.
    Zico's creativity and passion have profoundly shaped
    the way I think about academic problems and pursue
    research directions, and more broadly I have learned much
    more from him along the way.
    I am incredibly grateful for the immense
    amount of time and energy Zico has put into shaping the
    direction of this work and for molding me into who I am.

    Thanks to all of my close collaborators who have contributed
    to projects appearing in this thesis, including
    Byron Boots, Ivan Jimenez, Vladlen Koltun, Jacob Sacks, and Lei Xu,
    and more recently
    Akshay Agrawal,
    Shane Barratt,
    Stephen Boyd,
    Steven Diamond,
    and Brendan O'Donoghue.

    This thesis was also made possible by the great research
    environment that CMU has provided me during my studies here.
    CMU's collaborative, thriving, and understanding environment gave
    me the true capabilities to pursue my passions throughout my time here.
    I spent my first two years honing my systems skills working on
    wearable cognitive assistance applications with
    Mahadev (Satya) Satyanarayanan and am
    indebted to him for kindly giving me the freedom to pursue my
    interests in machine learning while part of his systems group.
    I hope that someday I will be able to pay this kindness forward.
    Thanks also to all of the administrative staff that have
    kept everything at CMU running smoothly, including
    Deb Cavlovich and Ann Stetser.
    I am also very thankful to Gaurav Manek for a well-engineered
    cluster setup that has made running and managing
    experiments effortless for the rest of us.
    And thanks to everybody else at CMU who have made
    graduate school incredibly enjoyable.
    These wonderful memories will stay with me for life.
    This includes
    Maruan Al-Shedivat,
    Alnur Ali,
    Filipe de Avila Belbute-Peres,
    Shaojie Bai,
    Sol Boucher,
    Noam Brown,
    Volkan Cirik,
    Dominic Chen,
    Zhuo Chen,
    Michael Coblenz,
    Jeremy Cohen,
    Jonathan Dinu,
    Priya Donti,
    Gabriele Farina,
    Benjamin Gilbert,
    Kiryong Ha,
    Jan Harkes,
    Wenlu Hu,
    Roger Iyengar,
    Christian Kroer,
    Jonathan Laurent,
    Jay-Yoon Lee,
    Lisa Lee,
    Chun Kai Ling,
    Stefan Muller,
    Vaishnavh Nagarajan,
    Vittorio Perera,
    Padmanabhan (Babu) Pillai,
    George Philipp,
    Aurick Qiao,
    Leslie Rice,
    Wolf Richter,
    Mel Roderick,
    Petar Stojanov,
    Dougal Sutherland,
    Junjue Wang,
    Phillip Wang,
    Po-Wei Wang,
    Josh Williams,
    Ezra Winston,
    Eric Wong,
    Han Zhao, and
    Xiao Zhang.

    My Ph.D.~would have been severely lacking without my internships
    at DeepMind in 2017 and Intel Labs in 2018.
    I learned how to craft large-scale reinforcement learning systems
    from Nando de Freitas and Misha Denil at DeepMind and
    about cutting-edge vision research from
    Vladlen Koltun at Intel Labs.
    Thank you all for hosting me.
    I am also grateful for all of the conversations and collaborations
    with the other interns and researchers in the industry as well,
    including
    Yannis Assael,
    David Budden,
    Serkan Cabi,
    Kris Cao,
    Chen Chen,
    Qifeng Chen,
    Yutian Chen,
    Mike Chrzanowski,
    Sergio Gomez Colmenarejo,
    Tim Cooijmans,
    Soham De,
    Laurent Dinh,
    Vincent Dumoulin,
    Tom Erez,
    Michael Figurnov,
    Jakob Foerster,
    Marco Fraccaro,
    Yaroslav Ganin,
    Katelyn Gao,
    Yang Gao,
    Caglar Gulcehre,
    Karol Hausman,
    Matthew W.~Hoffman,
    Drew Jaegle,
    David Lindell,
    Hanxiao Liu,
    Simon Kohl,
    Alistair Muldal,
    Alexander Novikov,
    Tom Le Paine,
    Ben Poole,
    Rene Ranftl,
    Scott Reed,
    German Ros,
    Evan Shelhamer,
    Sainbayar Sukhbaatar,
    Casper Kaae SÃ¸nderby,
    Brendan Shillingford,
    Yuval Tassa,
    Jonathan Uesato,
    Ziyu Wang,
    Abhay Yadav,
    Xuaner Zhang, and
    Yuke Zhu.

    I am grateful to the broader machine learning research community
    that has been thriving throughout my studies and has
    supported the direction of this work.
    This includes the Caffe, PyTorch, and TensorFlow communities
    I have interacted with over the years.
    These ecosystems have made the implementation and engineering
    side of this thesis easy and enjoyable.
    Thanks especially to Soumith Chintala, Adam Paszke, and the rest
    of the (Py)Torch community for helping me debug many strange
    errors and eventually contribute back.
    And thanks to everybody in the broader machine learning community
    who has given me deeper insights into problems or has graciously
    helped me with their code, including
    David Belanger,
    Alfredo Canziani,
    Alex Terenin, and
    Rowan Zellers.

    Thanks to all of the other communities that have provided me
    with the tooling and infrastructure necessary that allows
    me to work comfortably. These communities deserve more credit
    for the impacts that they have and the immense amount of
    development effort behind them and include the
    emacs \citep{stallman1981emacs},
    git \citep{torvalds2005git},
    hammerspoon,
    homebrew,
    \LaTeX \citep{lamport1994latex},
    Linux,
    mjolnir,
    mu4e,
    mutt,
    tmux,
    vim,
    xmonad \citep{stewart2007xmonad}, and
    zsh projects,
    as well as the many pieces of the Python ecosystem
    \citep{van1995python,oliphant2007python}, especially
    Jupyter \citep{kluyver2016jupyter},
    Matplotlib \citep{hunter2007matplotlib},
    seaborn,
    numpy \citep{van2011numpy},
    pandas \citep{mckinney2012python}, and
    SciPy \citep{jones2014scipy}.

    Looking back, my teachers and mentors earlier in my life
    ignited my interests in mathematics and computer science
    and opened my eyes.
    My high school teachers
    Suzanne Nicewonder,
    Susheela Shanta, and
    Janet Washington gave me a solid foundation
    in engineering and mathematics.
    Mack McGhee at Sunapsys hosted me for an
    internship that introduced to the wonderful
    world of Linux.
    Moving into my undergrad,
    Layne T.~Watson and David Easterling
    introduced me to the beautiful fields
    of optimization, numerical methods, and
    high-performance computing, and taught me how to
    write extremely optimized and robust Fortran code.
    I apologize for going to the dark side and writing
    ANTODL (another thesis on deep learning).
    Jules White and Hamilton Turner taught me how
    to hack Android internals and architect awesome Scala code.
    Binoy Ravindran, Alastair Murray, and Rob Lyerly
    taught me how to hack on compilers
    and the Linux kernel.

    On the personal side, I would like to thank all of my
    other friends, family members, and partners that
    have provided me with an immense amount of love,
    support, and encouragement throughout the years,
    especially Alice, Emma, and Nil-Jana.
    Thanks to my parents Sandy and David;
    brothers Chad and Chase;
    grandparents Candyth, Marshall, and Geneva;
    and the rest of my extended family
    for raising me in a wonderful environment and
    encouraging me at every step along the way.
    Thanks to my uncle Dan Dunlap for inspiring me and
    raving about AI, CS, philosophy, and music all of these years.
    And thanks to everybody else I have met in the
    arts,
    board games,
    climbing,
    cycling,
    dance,
    lifting,
    meditation,
    music,
    nature,
    poetry,
    theatre, and
    yoga
    communities in Pittsburgh, San Francisco, and London for
    providing a near-infinite amount of distractions from
    this thesis.
\end{acknowledgments}
% \restoregeometry

\pagestyle{plain}

\tableofcontents
\addtocontents{toc}{\vspace*{-2cm}}
\listoffigures
\addtocontents{lof}{\vspace*{-2cm}}
\listoftables
\listofalgorithms

\mainmatter

\include{sections/intro}
\include{sections/background}

\part{Foundations}
\include{sections/optnet}
\include{sections/icnn}

\part{Extensions and Applications}
\include{sections/cvxpyth}

\part{Conclusions and Future Directions}
\include{sections/conclusions}

\chapter*{Bibliography}
\addcontentsline{toc}{chapter}{Bibliography}

\vspace{-25mm}
This bibliography contains \total{citenum} references.
\vspace{10mm}

\printbibliography[heading=none]

\end{document}

%%% Local Variables:
%%% coding: utf-8
%%% mode: latex
%%% TeX-engine: xetex
%%% End:
